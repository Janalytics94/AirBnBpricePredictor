{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "written by: Jana Vihs, vihsjana@student.hu-berlin.de, 604930\n",
    "# Dear Jupyter Notebook Reader\n",
    "\n",
    "fancy, seeing you here. If you're planning on having a nice holiday in London soon, you came to the right place. \n",
    "Before we guide you through on how to use our Airbnb Price Predictor, we would like to explain a few details of the experimental design, but don't you worry that won't take long! After this only have to type one command into your terminal and you'll get the predicted price of your desired airbnb. \n",
    "\n",
    "If you like to know how this works in detail, stay tuned and read this  Jupyter Notebook till the end.\n",
    "\n",
    "Let's start!\n",
    "\n",
    "\n",
    "## A Word on the Experimental Design \n",
    "\n",
    "This Jupyter Notebook builds the core of the project and is intended to provide a common thread regarding data analysis, feature engineering and selection as well as model selection and evaluation of the final method. Nevertheless the main goal is to build a pipeline using .dvc in order to recieve price  predictions for an airbnb in London, United Kingdom [^1].\n",
    "\n",
    "### Tools\n",
    "\n",
    "As we are using a lot of new tools to develop our pipeline let us look at them a little more closely.\n",
    "\n",
    "#### Docker\n",
    "\n",
    "The whole project comes with a Dockerfile, which defines our project environment. You can use it but you don't have to. If you're a Docker Newbie checkout the following link https://docs.docker.com/get-started/ and make sure Docker is installed. \n",
    "Then run the following commands in your terminal to build a docker image and run the application inside a container.\n",
    "\n",
    "\n",
    "1. *docker build . -t airbnb* \n",
    "2. *docker run -it --name airbnb -v $(pwd):/root/ airbnb bash*\n",
    "\n",
    "Then you should be inside the docker container. To make sure all necessary dependencies are installed please run *pip install -r requirements.txt* inside the terminal of your docker container.\n",
    "\n",
    "If you don't want to use Docker that is totally fine, just run *pip install -r requirements.txt* to have all the packages available.\n",
    "\n",
    "#### .DVC\n",
    "DVC https://dvc.org/ is an amazing tool to develop data driven pipelines for ML-Projects. The designated pipeline is defined in the dvc.yaml file. Usually it includes all stages of an machine learning pipeline like data preprocessing, feature engineering etc. but display the function of DVC we only developed a small pipeline. If you would like to test it, please run \n",
    "*dvc init --no-scm* and then *dvc repro -f* in your terminal and you should see the price predction of the listing 0FEMC4VA5U. \n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "Here is an overview of the structure of this project:\n",
    "\n",
    "    ├── Dockerfile: Defines the environment.\n",
    "    ├── README.md: Top level documentation for developers.\n",
    "    ├── data\n",
    "    │ ├── external: Data from third party sources. \n",
    "    │ ├── interim: Intermediate data that has been transformed.\n",
    "    │ ├── canonical: Final data sets for modeling.\n",
    "    │ └── raw: The original, immutable data dump.\n",
    "    |\n",
    "    ├── dvc.yaml: Defines the data pipelines.\n",
    "    ├── models: Trained and serialized models, model predictions, or model summaries.\n",
    "    │\n",
    "    ├── notebooks: Jupyter notebook.\n",
    "    ├── requirements.txt: Requirements file python dependencies. \n",
    "    └── src: Source code for use in this project.\n",
    "        ├── __init__.py: Makes `src` a python module\n",
    "        ├── features: Scripts to turn raw data into features for modeling.\n",
    "        |–– ingest: Crawler to download, generate or add additional data. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[^1]: <small> Packaging (installation of the package via pip) of the module is omitted, but is theoretically possible </small>.\n",
    "\n",
    "As said before if you want to know how everything works in detail i hope you'll enjoy the following Notebook with the title:\n",
    "\n",
    "# The Airbnb Price Predictor - London Area\n",
    "\n",
    "### Table of Contents\n",
    "- Introduction\n",
    "    - Meta Information\n",
    "- Data Preprocessing\n",
    "    - Missing Values        \n",
    "- Explorative Data Analysis\n",
    "- Feature Engineering \n",
    "    - Hot Encoding\n",
    "    - Distance to City Center\n",
    "    - Host History\n",
    "    - Cancellation Policy\n",
    "    - Images\n",
    "    - Ammenities   \n",
    "    - Reviews\n",
    "    - Yelp Data\n",
    "- Benchmark Models\n",
    "    - Linear Regression\n",
    "    - Random Forest Regression\n",
    "    - XGBoost Regression\n",
    "    - LGBM\n",
    "    - Neural Network\n",
    "    - Performance\n",
    "- Final Method\n",
    "    - Hyperparameter Tuning\n",
    "- Conclusion and Outlook\n",
    "- References \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Airbnb, is a globally known peer-to-peer platform for short-term rental of housing accommodation.\n",
    "In this termpaper we want to try to forecast the price of an Airbnb based on specific features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages \n",
    "# Standards \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Visulaizations\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "sns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n",
    "sns.set_palette(palette=sns.color_palette(\"deep\"))\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Translator\n",
    "from google_trans_new import google_translator\n",
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Stats\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# ML - Machine Learning - Model Testing\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Evaluation\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import r2_score \n",
    "# DL - Deep Learning \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Grid Search \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# save\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change python path too include modules that i wrote myself\n",
    "sys.path.append(os.path.dirname('../src'))\n",
    "from src.features.preprocess.Processor import Processor\n",
    "from src.features.preprocess.Textprocessor import Textprocessor\n",
    "from src.features.preprocess.Evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor()\n",
    "# load data set \n",
    "train = pd.read_csv('../data/raw/train.csv', index_col='listing_id')\n",
    "test = pd.read_csv('../data/raw/test.csv', index_col='listing_id')\n",
    "reviews = pd.read_csv('../data/raw/reviews.csv', index_col='listing_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Information \n",
    "\n",
    "The designated data set is available on Kaggle https://www.kaggle.com/c/adams2021/data and contains the tabular as well as text features which describe each Airbnb listing.\n",
    "The main goal is to predict the price for a future Airbnb listing based on those properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our train data consists of {}'.format(train.shape[0]) + ' rows and {}'.format(train.shape[1]) + ' columns, while our test data contains {}'.format(test.shape[0]) + ' rows and {}'.format(test.shape[1]) + ' columns.')\n",
    "print('The additional data set reviews consist of {}'.format(reviews.shape[0]) + ' rows and {}'.format(reviews.shape[1]) + ' columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We start with our data preprocessing by calling our defined Processor.\n",
    "First of all we will change the data types of our dataframes due to memory reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call processor\n",
    "processor = Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  change data types because of memory reasons\n",
    "train = processor.change_data_types(train)\n",
    "test = processor.change_data_types(test)\n",
    "reviews = processor.change_data_types(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to drop neighbourhood right away as we already have neighbourhood_cleansed, also space and summary as description is a mixture of both. The picture_url will be used in the Image Crawler to crawl the additional image data, so we won't need it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = processor.drop_features(train)\n",
    "test = processor.drop_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "As the following table shows we have a few missing values in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in test data\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values  in train data \n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some features we will just fill them up with a one because their has to be a bathroom, at least one bedroom and one bed even if the bedroom is a living room and the bed is a couch, why then would you need an Airbnb, right ? With the zipcodes we use geopy and get the missing values from latitude and longitude, using our Processor in Processor.py. During our analysis and filling up the missing values of the zipcodes with the ones that we got from geopy we found out that exactly 25 zipcodes in the training and 10 in the test set could not be found. Hence, we drop those in the train data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis\n",
    "\n",
    "To get a better grip of our data we will explore each feature during our explorative data analysis. We will start iwth target variable *price* and then we will take a look at the relationship between the additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The mean average price of an airbnb is {}'.format(round(train.price.mean(),2)) + ' £.')\n",
    "print('The median price of an airbnb is {}'.format(round(train.price.median(),2)) + ' £.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution and Box Plot\n",
    "f, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "sns.distplot(train.price, rug=True, ax=axes[0]).set_title('Distribution of Price Data')\n",
    "print(\"Skewness: %f\" % train['price'].skew())\n",
    "print(\"Kurtosis: %f\" % train['price'].kurt())\n",
    "plt.title('Box Plot of Price Data')\n",
    "sns.boxplot(train.price, ax=axes[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable seems to be right-skewed, so we are going to apply log transformation to the price data. The box plot shows that we have a few outliers in the 100 range what would explain the high mean average price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Price Data after Log Trans.\n",
    "train['log_price'] = train['price'].apply(lambda x: processor.price_log_transformation(x))\n",
    "print(\"Skewness: %f\" % train['log_price'].skew())\n",
    "print(\"Kurtosis: %f\" % train['log_price'].kurt())\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Price Distribution after Log Transformation\")\n",
    "sns.distplot(train.log_price, rug=True, fit=scipy.stats.norm)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distrbution plot of the price data after the log-transformation leads us to the assumption that the price data might be following a normal distrbution which follows the following formula:\n",
    "$$ \n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \n",
    "  \\exp\\left( -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{\\!2}\\,\\right).\n",
    "$$,\n",
    "\n",
    "We are going to perform a Shapiro-Wilk Test in order to test our hypothesis. Since it is only designed to examine samples of size $ 3 \\leq n \\leq 50$ , we will draw 20 samples randomly from our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk Test\n",
    "random.seed(123)\n",
    "stat, p = shapiro(train.log_price.sample(n=20))\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to cluster the price data using the K-Means Algorithm. We want to devide out data set into three different price clusters: affordable, middle and expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS\n",
    "random.seed(123)\n",
    "price = np.array(train.price)\n",
    "price = price.reshape(-1,1)\n",
    "kmeans = KMeans(n_clusters=3).fit(price.reshape(-1,1))\n",
    "price_labels = kmeans.predict(price.reshape(-1,1))\n",
    "centers = kmeans.cluster_centers_\n",
    "train['price_cluster'] = price_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Cluster\n",
    "random.seed(123)\n",
    "middle = train[train['price_cluster']==0]\n",
    "affordable = train[train['price_cluster']==1]\n",
    "expensive = train[train['price_cluster']==2]\n",
    "train['price_cluster'] = train['price_cluster'].map({0 : 'affordable', 1: 'middle', 2:'expensive'})\n"
   ]
  },
  {
   "source": [
    "As we can see in the follwoing box plot, we can identify three different price cluster.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots Price Cluster\n",
    "random.seed(123)\n",
    "plt.title('Box Plots Price Cluster')\n",
    "ax = sns.boxplot(x=\"price_cluster\", y=\"price\", data=train)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "The following Scatterplot which displays the different location of each Airbnb using longitude and latitude according to their price distribution, reveals that there seem no difference in their price whether the place is in the city.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Airbnbs and their price clusters according lonitude and lattitude\n",
    "plt.dims=(12,10)\n",
    "plt.figure(figsize=plt.dims)\n",
    "sns.scatterplot(x='longitude', y='latitude', hue='price_cluster', data=train, palette='deep', size=train.price_cluster,sizes=(200, 20), legend=\"full\")\n",
    "plt.legend()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Correlation Plot reveals, that the different properties of each airbnb either have a strong positve correlation or a rather small negative correlation.\n",
    "Esspecially the features *accommodates, bathrooms, beds, bedrooms and guests_included* have a strong correlation with the price data. Makes sense, with size of the airbnb the price rises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "plt.figure(figsize=(15,15))\n",
    "corr = train.select_dtypes(['int32', 'float32', 'int64']).corr()\n",
    "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':10}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.dims=(15,15)\n",
    "plt.figure(figsize=plt.dims)\n",
    "sns.scatterplot(train.longitude, train.latitude, hue=train.neighbourhood_cleansed, size=train.price_cluster,sizes=(200, 20), legend=\"full\")\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map\n",
    "#lonlat = list(zip(train.longitude, train.latitude))\n",
    "#mapit = folium.Map( location=[52.667989, -1.464582], zoom_start=6 )\n",
    "#for coord in lonlat:\n",
    "#    folium.Marker( location=[ coord[0], coord[1] ], fill_color='#43d9de', radius=8 ).add_to( mapit )\n",
    "\n",
    "#mapit.save( 'map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We certainly applied a lot of feature engineering to our data set. In this section we will summarize, what inform\n",
    "\n",
    "### Hot Encoding \n",
    "\n",
    "We will use one hot encoding for the following data features bed_type, property_type and room_type.\n",
    "\n",
    "\n",
    "### Distance to City Center\n",
    "\n",
    "If you are analysing a bigger city that has multiple locations that are considered desirable, you can also run this code as many times as needed with different geographical points. (Don’t forget to change the column names so you don’t overwrite the previous point!).\n",
    "For example, there is a financial district close to the Amsterdam Zuid station that could be equally (or even more) relevant to working tenants than living close to the city center. Measuring these various scenarios is more important if you are using methods similar to multiple linear regressions rather than machine-learning statistical algorithms because they are inherently better at recognising non-linear relationships and clusters. For this reason, I won’t include it in this analysis but it is an interesting factor to weight in depending on the statistical method being used. As Airbnb is mainly used fo holiday i would say hence we chose a pretty touristic place\n",
    "**Distance to City Center**. We have chosen the Picadally Circus as city center, but using other coordinates might also be possible.\n",
    "\n",
    "\n",
    "![Picadally Circus](../data/external/londonpiccadillycircus.jpg) \n",
    "\n",
    "### Host History\n",
    "\n",
    "Since we have our feature *host_since* in our dataframe we can calculate how long the host is acutally registered at the platform, which also might be a great additional feature for the price prediciton. As a user it might be important to increase his trust into the airbnb offer, if a host has gained some experience, offering Airbnb's. *The host_response_rate* was converted into probabilities. \n",
    "\n",
    "## Cancellation Policy\n",
    "\n",
    "More information on the Cancellation policy can be found on https://www.airbnb.de/home/cancellation_policies#long-term. The types of the cancellation polices were summarized into different groups: light, moderate and strict. \n",
    "\n",
    "\n",
    "### Images\n",
    "\n",
    "Thanks to the feature *picture_url* we are able to crawl all the images, so we actually are able to see how the Airbnb looks like.\n",
    "The code can be found in **/root/src/ingest/ImageCrawler.py** and the actual images will be stored in **/root/data/images/** in the designated directories regarding test and train data. The images themselves will be processed using our **ImageProcessor**. To keep our scope narrow, we will pull four values from the image that may be of value: the brightness and the RGB values, hence the number of red, blue and green pixels. \n",
    "\n",
    "** Why could this be important ? **\n",
    "\n",
    "Taking a step back, we can think of a few things in the image that may impact the price:\n",
    "\n",
    "    - Are the floors carpet or hardwood?\n",
    "    - Are the walls painted or wallpaper? Paintings or posters?\n",
    "    - Does the host have plants? Are they alive and healthy?\n",
    "    - Is the picture well-lit and inviting?\n",
    "\n",
    "\n",
    "### Ammenities\n",
    "\n",
    "The different ammenities were cleaned, recoded, then encoded and added to our data.\n",
    "\n",
    "### Reviews\n",
    "\n",
    "In order to analyze our text data we will use **spacy** https://spacy.io/. \n",
    "The reviews and the text data in general contains valuable information so we dedicated a whole chapter to it.\n",
    "\n",
    "\n",
    "### Yelp Data\n",
    "\n",
    "As we are going for holiday to London we of course want to find out about all the yummy restaurants. Also we want to stay close to the busiest places to actually see London, so being close to the restaurants and markets and shops might have an impact to the price of airbnb.\n",
    "In order to find that out we scraped data using the Yelp API https://www.yelp.com/developers.\n",
    "Sadly, we could not collect enough data for our scope but it would be definetely worth a try for future research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('../data/external/yelp.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Review Data\n",
    "\n",
    "Our data contains reviews in different languages like spanish, french, german or different languages from asia. Also we have reviews, that only contain one character or use a Thumpsup to express a positive opinion. Usually we would remove those data in our text cleaning process but in that case, we may lose valuable information if we remove the emojis. In this case, a better approach is to convert emoji to word format so that it preserves the emoji information.\n",
    "\n",
    "The following cleaning steps will be performed on our review data:\n",
    "\n",
    "1.Step :   Remove unnecessary characters like \\r, \\n, urls and further            that might reduce prediction power.\n",
    "\n",
    "2.Step :   Translation of non-englisch reviews using google translator            python package.\n",
    "\n",
    "3.Step :   Converting emojis into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in processed review df\n",
    "reviews = pd.read_csv('../data/interim/reviews/csv/reviews.csv', index_col='listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Textprocessor\n",
    "textprocessor = Textprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textprocessor.clean(reviews.comments[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(reviews.comments[5])\n",
    "tokens = [token for token in doc ]"
   ]
  },
  {
   "source": [
    "Now, we will remove any stopwords, punctuations and white spaces that are still left in our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "filtered = [token for token in tokens if not token.is_stop]\n",
    "# remove puntuations\n",
    "filtered = [token for token in filtered if not token.is_punct]\n",
    "# remove white spaces \n",
    "filtered = [token for token in filtered if not token.is_space ]\n",
    "# lemmatize and turn it to lowercase\n",
    "lemmas = [token.lemma_.strip().lower() for token in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_freq = Counter(lemmas)\n",
    " # 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unique words\n",
    "unique_words = [lemmas for (lemma, freq) in word_freq.items() if freq == 1]\n",
    "#print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "wordcloud = WordCloud().generate(reviews.lemmas[0])\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "The following pie chart just gives a quick overview of the proportion of languages in our review data. Well, english has by far the mayority."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview languages\n",
    "plt.figure(figsize=(15,15))\n",
    "lan = reviews['language'].value_counts()[:5]\n",
    "labels = lan.keys()\n",
    "lan.plot.pie(autopct=\"%.1f%%\", explode=[0.05]*5, labels=labels, pctdistance=0.5)\n",
    "plt.title(\"Languages\", fontsize=14);"
   ]
  },
  {
   "source": [
    "Then we detected whether a reveiw was positive or negative. The following Distribution Plot displays the polarity or popularity of each airbnb listing and the subjectivity of each review. It makes sense, that they are both equally distributed as an opinion about something is always subjective. A polarity value below zero indicates a negative review while a value above zero indicates a positive review. All in all most of the airbnb's that received reviews at all are evaluated very positively. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Subjectivity and Polarity in review data\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Distribution of Polarity and Subjectivity\")\n",
    "sns.distplot(reviews['polarity'])\n",
    "sns.distplot(reviews['subjectivity'])\n",
    "plt.legend(labels=['Polarity', 'Subjectivity'])\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preparation\n",
    "\n",
    "Now we start with our feature prepearation for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned data frames \n",
    "train = pd.read_csv('../data/canonical/train/train.csv', index_col='listing_id')\n",
    "test = pd.read_csv('../data/canonical/test/test.csv', index_col='listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our merged train data consists of {}'.format(train.shape[0]) + ' rows and {}'.format(train.shape[1]) + ' columns, while our merged test data contains {}'.format(test.shape[0]) + ' rows and {}'.format(test.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.price\n",
    "target_log = train.log_price\n",
    "train = train.drop(['log_price', 'price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(target_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set \n",
    "x_train, x_test, y_train, y_test = processor.create_train_validation_frames(train, target_log, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes again\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmarking\n",
    "\n",
    "We will try out different models like Linear Regression, Random Forest Regression, XGBoost for Regression and LGBM.\n",
    "\n",
    "We will evaluate each model using the following metrics :\n",
    "\n",
    "$R^{2}$, the MSLE (as our target feature is arleady log-transformed we only have to calculate the MSE and use the reverse function $\\exp(x)$ to calculate the usual MSE of our price data), RMSE and SMAPE.\n",
    "\n",
    "We also check the feature importances of each model in order to find out which properties of each airbnb have the most influence on the price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluator\n",
    "evaluate = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X= True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "reg = reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use  Model to predict values\n",
    "y_pred = reg.predict(x_train)\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Training Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % reg.score(x_train,y_train))\n",
    "print(\"Mean Squared Log Error : %0.3f\" % mean_squared_error(y_train,y_pred))\n",
    "print(\"Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred)))\n",
    "print(\"Root Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred))**0.5)\n",
    "print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_train),np.exp(y_pred)))\n",
    "\n",
    "#predictions = pd.Series(predictions, index=validate_.index, name='price')\n",
    "#predictions = predictions.apply(lambda x : np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use  Model to predict values\n",
    "y_pred = reg.predict(x_test)\n",
    "r2_reg = reg.score(x_test,y_test)\n",
    "msle_reg = mean_squared_error(y_test,y_pred)\n",
    "mse_reg = mean_squared_error(np.exp(y_test),np.exp(y_pred))\n",
    "rmse_reg = mean_squared_error(np.exp(y_test),np.exp(y_pred))**0.5\n",
    "smape_reg = evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred))\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Test Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % r2_reg)\n",
    "print(\"Mean Squared Log Error : %0.3f\" % msle_reg)\n",
    "print(\"Mean Squared Error : %0.3f\" % mse_reg)\n",
    "print(\"Root Mean Squared Error : %0.3f\" % rmse_reg)\n",
    "print(\"SMAPE : %0.3f \" % smape_reg )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Casual and Registered model's residuals:\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "sns.regplot(np.exp(y_test),np.exp(y_pred), line_kws={\"color\": \"red\"})\n",
    "plt.title(\"Residuals for Linear Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_coef = pd.DataFrame(list(zip(train.columns.tolist(),(reg.coef_))),columns=['Feature','Coefficient'])\n",
    "lin_reg_coef.sort_values(by='Coefficient',ascending=False).iloc[:50]"
   ]
  },
  {
   "source": [
    "### Random Forest Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(max_depth=10, n_estimators=100)\n",
    "\n",
    "#Train the classifier\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "#Plot variable importances for the top 10 predictors\n",
    "importances = clf.feature_importances_\n",
    "feat_names = train.columns.tolist()\n",
    "tree_result = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
    "tree_result.sort_values(by='importance',ascending=False)[:10].plot(x='feature', y='importance', kind='bar',color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Training Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % clf.score(x_train,y_train))\n",
    "print(\"Mean Squared Log Error : %0.3f\" % mean_squared_error(y_train,y_pred))\n",
    "print(\"Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred)))\n",
    "print(\"Root Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred))**0.5)\n",
    "print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_train),np.exp(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to predict values\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "r2_rf = clf.score(x_test,y_test)\n",
    "msle_rf = mean_squared_error(y_test,y_pred)\n",
    "mse_rf = mean_squared_error(np.exp(y_test),np.exp(y_pred))\n",
    "rmse_rf = mean_squared_error(np.exp(y_test),np.exp(y_pred))**0.5\n",
    "smape_rf = evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred))\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Test Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % r2_rf)\n",
    "print(\"Mean Squared Log Error : %0.3f\" % msle_rf)\n",
    "print(\"Mean Squared Error : %0.3f\" % mse_rf)\n",
    "print(\"Root Mean Squared Error : %0.3f\" % rmse_rf)\n",
    "print(\"SMAPE : %0.3f \" % smape_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Casual and Registered model's residuals:\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "sns.regplot(np.exp(y_test),np.exp(y_pred),line_kws={\"color\": \"red\"})\n",
    "plt.title(\"Residuals for Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlf = XGBRegressor()\n",
    "xlf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use  Model to predict values\n",
    "y_pred = xlf.predict(x_train)\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Training Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % xlf.score(x_train,y_train))\n",
    "print(\"Mean Squared Log Error : %0.3f\" % mean_squared_error(y_train,y_pred))\n",
    "print(\"Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred)))\n",
    "print(\"Root Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred))**0.5)\n",
    "print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_train),np.exp(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to predict values\n",
    "y_pred = xlf.predict(x_test)\n",
    "\n",
    "r2_xgb = xlf.score(x_test,y_test)\n",
    "msle_xgb = mean_squared_error(y_test,y_pred)\n",
    "mse_xgb = mean_squared_error(np.exp(y_test),np.exp(y_pred))\n",
    "rmse_xgb = mean_squared_error(np.exp(y_test),np.exp(y_pred))**0.5\n",
    "smape_xgb = evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred))\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Test Data\")\n",
    "print(\"R^2 value using score fn: %.3f\" % r2_xgb)\n",
    "print(\"Mean Squared Log Error : %0.3f\" % msle_xgb)\n",
    "print(\"Mean Squared Error : %0.3f\" % mse_xgb)\n",
    "print(\"Root Mean Squared Error : %0.3f\" % rmse_xgb)\n",
    "print(\"SMAPE : %0.3f \" % smape_xgb)"
   ]
  },
  {
   "source": [
    "### LGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(xtrain20, y_train)\n",
    "lgb_eval = lgb.Dataset(xtest20, y_test, reference=lgb_train)\n",
    "feat_names = xtrain20.columns.tolist()\n",
    "#Config the LGBM model parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=300,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5,\n",
    "               evals_result=evals_result,\n",
    "               feature_name=feat_names)\n",
    "\n",
    "print('Saving model...')\n",
    "#Save the fit model to a file\n",
    "gbm.save_model('../data/models/model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values\n",
    "y_pred = gbm.predict(xtrain20, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Training Data\")\n",
    "\n",
    "print(\"Mean Squared Log Error : %0.3f\" % mean_squared_error(y_train,y_pred))\n",
    "print(\"Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred)))\n",
    "print(\"Root Mean Squared Error : %0.3f\" % mean_squared_error((y_train),(y_pred))**0.5)\n",
    "print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error((y_train),(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values\n",
    "y_pred = gbm.predict(xtest20, num_iteration=gbm.best_iteration)\n",
    "\n",
    "msle_gbm = mean_squared_error(y_test,y_pred)\n",
    "mse_gbm = mean_squared_error(np.exp(y_test),np.exp(y_pred))\n",
    "rmse_gbm = mean_squared_error(np.exp(y_test),np.exp(y_pred))**0.5\n",
    "smape_gbm = evaluate.symmetric_mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred))\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Test Data\")\n",
    "print(\"Mean Squared Log Error : %0.3f\" % msle_gbm)\n",
    "print(\"Mean Squared Error : %0.3f\" % mse_gbm)\n",
    "print(\"Root Mean Squared Error : %0.3f\" % rmse_gbm)\n",
    "print(\"SMAPE : %0.3f \" % smape_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting metrics recorded during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "plt.show()\n",
    "\n",
    "print('Plotting feature importances...')\n",
    "ax = lgb.plot_importance(gbm, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Casual and Registered model's residuals:\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "sns.regplot((y_test),(y_pred), line_kws={'color': 'red'})\n",
    "plt.title(\"Residuals for LGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all to float32\n",
    "x_train=np.asarray(x_train).astype(np.float32)\n",
    "y_train=np.asarray(y_train).astype(np.float32)\n",
    "x_test=np.asarray(x_test).astype(np.float32)\n",
    "y_test=np.asarray(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#Step1. Define the model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(16, activation = 'relu', kernel_initializer = 'he_normal', input_shape = (x_train.shape[1],)))\n",
    "model.add(Dense(8, activation = 'relu', kernel_initializer = 'he_normal'))\n",
    "model.add(Dense(4, activation = 'relu', kernel_initializer = 'he_normal'))\n",
    "model.add(Dense(2, activation = 'relu', kernel_initializer = 'he_normal'))\n",
    "model.add(Dense(1))\n",
    "#Step2. Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'rmse', metrics = 'rmse')\n",
    "#Step3. Fit the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10,batch_size=64, verbose=0)\n",
    "#Step4.1 Evaluate the model\n",
    "loss, mae = model.evaluate(x_test, y_test)\n",
    "#Step4.2 Plot the learning curve\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.2)\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks = [lr_reducer, early_stopper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train ,y_train, validation_data=(x_test, y_test), callbacks=callbacks, epochs=10, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Training Data\")\n",
    "print(\"Mean Squared Log Error : %0.3f\" % mean_squared_error(y_train,y_pred))\n",
    "print(\"Mean Squared Error : %0.3f\" % mean_squared_error(np.exp(y_train),np.exp(y_pred)))\n",
    "print(\"Root Mean Squared Error : %0.3f\" % mean_squared_error((y_train),(y_pred))**0.5)\n",
    "#print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error((y_train),(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "msle_nn = mean_squared_error(y_test,y_pred)\n",
    "mse_nn = mean_squared_error(np.exp(y_test),np.exp(y_pred))\n",
    "rmse_nn = mean_squared_error((y_test),(y_pred))**0.5\n",
    "# Calculate the Mean Squared Error using the mean_squared_error function.\n",
    "print(\"Test Data\")\n",
    "print(\"Mean Squared Log Error : %0.3f\" % msle_nn)\n",
    "print(\"Mean Squared Error : %0.3f\" %  mse_nn)\n",
    "print(\"Root Mean Squared Error : %0.3f\" % rmse_nn)\n",
    "#print(\"SMAPE : %0.3f \" % evaluate.symmetric_mean_absolute_percentage_error((y_test),(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of Casual and Registered model's residuals:\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "sns.regplot((y_test),(y_pred), line_kws={'color': 'red'})\n",
    "plt.title(\"Residuals for the Neural Network\")"
   ]
  },
  {
   "source": [
    "### Performance \n",
    "\n",
    "The following performance summarizes the evaluation metrics of each model we tried."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for test data\n",
    "models_used = ['Linear Regression', 'Random Forest Regressor', 'XGB Regressor', 'LGBM Regressor', 'Simple Neural Network']\n",
    "\n",
    "performance_metrics = {'R^2': [r2_reg,r2_rf,r2_xgb,'-','-'], 'MSE':[mse_reg,mse_rf,mse_xgb,mse_gbm,mse_nn], 'RMSE':[rmse_reg,rmse_rf,rmse_xgb,rmse_gbm,rmse_nn], 'MSLE': [msle_reg,msle_rf,msle_xgb,msle_gbm,msle_nn], 'SMAPE':[smape_reg,smape_rf,smape_xgb,smape_gbm,'-']}\n",
    "\n",
    "pd.DataFrame(performance_metrics, index=models_used)"
   ]
  },
  {
   "source": [
    "A Deep Learning Model is known to perform rather poor on tabular data, so the text features could have been separated and fed into a Neural Network while the tabular data will be fed to our final method. At the end, both predictions could be combined to receive a final result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Final Method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search \n",
    "gsc = GridSearchCV(\n",
    "            estimator=xlf,\n",
    "            param_grid={\"learning_rate\": (0.05, 0.10, 0.15),\n",
    "                        \"max_depth\": [ 3, 4, 5, 6, 8],\n",
    "                        \"min_child_weight\": [ 1, 3, 5, 7],\n",
    "                        \"gamma\":[ 0.0, 0.1, 0.2],\n",
    "                        \"colsample_bytree\":[ 0.3, 0.4],},\n",
    "            cv=3, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "grid_result = MultiOutputRegressor(gsc).fit(x_train,np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "best_params = grid_result.estimators_[0].best_params_  # for the first y_target estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb = xgboost.DMatrix(x_train, y_train)\n",
    "test_xgb = xgboost.DMatrix(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"colsample_bytree\":0.4, \n",
    "    \"gamma\":0.2,\n",
    "    \"learning_rate\":0.1,\n",
    "    \"max_depth\":8, \n",
    "    \"min_child_weight\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model = XGBRegressor(\n",
    "    colsample_bytree=0.4,\n",
    "    gamma=0.2, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=8,\n",
    "    min_child_weight=3\n",
    ")\n",
    "model.fit(x_train, y_train, \n",
    "          eval_set=[(x_train, y_train), (x_test, y_test)], \n",
    "          early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test, ntree_limit=model.best_ntree_limit)\n",
    "predictions = pd.Series(predictions.ravel(), index=test.index, name='price')\n",
    "predictions = predictions.apply(lambda x: np.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of proce predicitons\n",
    "plt.title('Distribution of Price Predictions')\n",
    "sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preds\n",
    "predictions.to_csv('../predictions/sample_submission_604930.csv')\n",
    "# save model\n",
    "pickle.dump(model, open(\"../data/models/xgb_reg.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Outlook\n",
    "\n",
    "In the scope of this termpaper many approaches came to our mind and it still offers so much room for improvement. As we included the review data and the image data in our model and hoped for great improvement in our prediciton power which was sadly not the case, we have refrained from adding any further text features, which should be done differntly in the future. \n",
    "The following list shall give an overview what kind of approaches could be used to improve our Airbnb-Price-Predictor:\n",
    "\n",
    "    1. Use Bert on the description data of the airbnb to get word embeddings and maybe try to add a regression layer to predict the price\n",
    "    2. Use an Object Detection CNN like Yolo or an CNN for Image Processing on the actual crawled pictures of each airbnb to add additonal features\n",
    "    3. Use the yelp data to get additonal features like explained in the section *Feature Engineering*\n",
    "    4. Try to cluster the numeric features to get more information about each price cluster. It might be worth a try to transform our current regression problem into a classification problem to detect different price cluster and then predict a price for each segment.\n",
    "    5. Get even more information on each Airbnb listing. Usually it is listed if one has to pay a cleaning fee which might be important feature as well\n",
    "\n",
    "There is always so much more you can do!"
   ]
  },
  {
   "source": [
    "## References\n",
    "\n",
    "1.   Kincl, Tomáš & Novák, Michal & Pribil, Jiri. (2016). Sentiment Classification in Multiple Languages: Fifty Shades of Customer Opinions. 10.1007/978-3-319-22593-7_19. \n",
    "\n",
    "2.   Patel, Miral & Darji, Mittal & janki,. (2018). Stock Price Prediction Using Clustering and Regression: A Review. \n",
    "\n",
    "3.   Rezazadeh, Pouya & Nikolenko, Liubov & Rezaei, Hoormazd. (2019). Airbnb Price Prediction Using Machine Learning and Sentiment Analysis. \n",
    "\n",
    "4.  Trivedi, Shubhendu & Pardos, Zachary & Heffernan, Neil. (2015). The Utility of Clustering in Prediction Tasks. \n",
    "\n",
    "5.  Xing, Yazhou & Qian, Zian & Chen, Qifeng. (2021). Invertible Image Signal Processing. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}